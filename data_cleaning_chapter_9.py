# -*- coding: utf-8 -*-
"""Data Cleaning chapter 9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DIBRMG6x5-RcgCXObPzVMhp5Vy-EY3sy
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

nls97 = pd.read_csv("nls97.csv")
nls97.set_index('personid' , inplace = True)

# convert the all the column with the object dtype in the NLS data to category data type.
# Do this selecting object datatype columns with select_stype and using apply plus a lanbda function to change  the datatype  to 'category'

nls97.loc[:, nls97.dtypes == 'object'] = nls97.select_dtypes(['object']).apply(lambda x: x.astype('category'))

# select the column using  tha pandas [] bracket operator and the loc and iloc accesors

analysisdemo = nls97['gender']
type(analysisdemo)

analysiddemo = nls97[['gender']]
type(analysisdemo)

analysisdemo = nls97.loc[:,['gender']]
print(type(analysisdemo))

analysisdemo = nls97.iloc[:,[0]]
print(type(analysisdemo))

# select the multiple column from pandas data frame
analysisdemo = nls97[['gender', 'maritalstatus', 'highestgradecompleted']]
analysisdemo.shape

analysisdemo.head()

analysisdemo2 = nls97.loc[:, ['gender', 'maritalstatus','highestgradecompleted']]
analysisdemo2.shape

analysisdemo2.head()

# Select the multiple column based on a list of columns

keyvars = ['gender', 'maritalstatus', 'highestgradecompleted', 'wageincome', 'gpaoverall','weeksworked17','colenroct17']
analysiskeys = nls97[keyvars]
analysiskeys.info()

# select one or more columns by filtering on column name

analysiswork = nls97.filter(like = "weeksworked")
analysiswork.info()

# select the all the column with the category data type

analysiscats = nls97.select_dtypes(include = ['category'])
analysiscats.info()

# select the columns with numeric data types

 analysisnums = nls97.select_dtypes(include = ['number'])
 analysisnums.info()

# organize column using list of column names



"""# ***Selecting Rows***

When we are taking the measure of our data and otherwise answering the question, "How does it look?", we
are constantly zooming in and out. We are looking at aggregated numbers and particular rows. But there are
also important data issues that are only obvious at an intermediate zoom level, issues that we only notice
when looking at some subset of rows. This recipe demonstrates how to use the pandas tools for detecting
data issues in subsets of our data.
"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv('nls97.csv')
nls97.set_index('personid', inplace = True)

nls97[1000:1004].T

# use slicing to start at the 100st row and go to the 1004th row skipping evevry other row

nls97[1000: 1004:2].T

# Select first row using  head and []

nls97.head(3).T

nls97[:3].T

# select the last three row using tail and [] operator slicing

nls97.tail(3).T

nls97.loc[[195884,195891, 195970 ]].T

# Select the row from the beggining of the data frame with the iloc succesor

nls97.iloc[[0]].T

# Select the feww rows from the beggining of the dataframe  with the  ilocdata succesor

nls97.iloc[[0,1,2]].T

# select the few row from the end of the data frame with the iloc data accessor

nls97.iloc[[-1,-2,-3]].T

# select multiple rows conditionally using boolean indexing

nls97.nightlyhrssleep.quantile(0.05)

nls97.nightlyhrssleep.count()

sleepcheckbool = nls97.nightlyhrssleep <=4
sleepcheckbool

lowsleep = nls97.loc[sleepcheckbool]
lowsleep.info()

# select rows based on multiple conditions

lowsleep3pluschildren =  lowsleep.loc[lowsleep.childathome >=3]
lowsleep3pluschildren.T

lowsleep.childathome.describe()

lowsleep3pluschildren = nls97.loc[(nls97.nightlyhrssleep <= 4) & (nls97.childathome >= 3)]
lowsleep3pluschildren.shape

# select rows and columns based on multiple condition

lowsleep3pluschildren = nls97.loc[(nls97.nightlyhrssleep <= 4) & (nls97.childathome >=3),['nightlyhrssleep','childathome']]
lowsleep3pluschildren



"""# **Generating frequencies for categorical variables**"""

import pandas as pd
import numpy as np
nls97 = pd.read_csv("nls97.csv")

nls97.set_index('personid', inplace = True)
nls97.loc[:, nls97.dtypes == 'object'] = nls97.select_dtypes(['object']).apply(lambda x : x.astype('category'))

# show the names of the columns with the category data type and check the for number of missing value

catcols = nls97.select_dtypes(include = ['category']).columns
catcols

nls97[catcols].isnull().sum()

# show the frequencies for marital status

  nls97.maritalstatus.value_counts()

# turn off sorting  by frequency

nls97.maritalstatus.value_counts(sort = False )

#show precentage instead of count
nls97.maritalstatus.value_counts(sort = False, normalize = True)

# show the percentage for all goverenment responsibility columns

#Filter the DataFrame for just the government responsibility columns, then use apply to run
#value_counts on all columns in that DataFrame:

nls97.filter(like = 'gov').apply(pd.value_counts, normalize = True)

# Find the percentage for all goverenment  responsibility columns of people who are married

# first select only rows with the maritalstatus equal to Married

nls97[nls97.maritalstatus == "Married"].filter(like = 'gov').apply(pd.value_counts, normalize = True)

#find the frequncies and percentages for all category columns in the DataFrame
  # print the details into the text file

freqout = open('frequencies.txt', 'w')

for col in nls97.select_dtypes(include = ['category']):
  print(col, "--------------------", "frequencies",nls97[col].value_counts(sort = False),
        "percentages", nls97[col].value_counts(normalize = True, sort = False),\
        sep= "\n\n", end ="\n\n\n", file = freqout)

freqout.close()



"""# ***Generating summary statics for continuous variables***"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

covidtotals = pd.read_csv('covidtotals.csv', parse_dates = ['lastdate'])
covidtotals.set_index('iso_code',inplace = True)

# let's remind overselves of the structure of the data

covidtotals.shape

covidtotals.sample(2, random_state= 1).T

covidtotals.dtypes

# get the decriptive statistics on the covid totals and demogrpahic columns

covidtotals.describe()

# Take a closer look at the distribution of values for the cases and deaths columns

totvars = ['location', 'total_cases', 'total_deaths', 'total_cases_pm', 'total_deaths_pm']
covidtotals[totvars].quantile(np.arange(0.0, 1.1, 0.1))

# view the total distribution of total cases

plt.hist(covidtotals['total_cases']/1000, bins = 12)
plt.title("Total Covid cases")
plt.xlabel("Cases")
plt.ylabel("Number of countries")
plt.show



"""# ***CHPATER 04***

##finding missing values
"""

# this  code is  not belong to above lerning series...

sales_record = {'price': 1.24, 'num_item':4 , 'person': 'chirs'}
sales_statement = '{} bought {} items at a price of {} each for a total of {}'

print(sales_statement.format(sales_record['person'], sales_record['num_item'], sales_record['price'],sales_record['price']*sales_record['num_item'] ))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

covidtotals =  pd.read_csv('covidtotalswithmissings.csv')
totavars = ['location','total_cases', 'total_deaths','total_cases_pm', 'total_deaths_pm']
demovars = ['population', 'pop_density', 'median_age', 'gdp_per_capita', 'hosp_beds']

# check the demogrpahhic colum data for missing data

covidtotals[demovars].isnull().sum(axis = 0)

demovarsmisscnt = covidtotals[demovars].isnull().sum(axis = 1)
demovarsmisscnt.value_counts()

# list the countries with three or more missing values for the demograpphic data

covidtotals.loc[demovarsmisscnt >=3, ['location'] + demovars].head(10).T

# check the covid cases data for missing values

covidtotals[totavars].isnull().sum(axis = 0)

totavarmisscnt = covidtotals[totavars].isnull().sum(axis = 1)
totavarmisscnt.value_counts()

covidtotals.loc[totavarmisscnt > 0].T

# use the fillna method to fix the missing data cases data for the one country affected [Honk kong]

# // we can fill the missing values from the other particular details //

covidtotals.total_cases_pm.fillna(covidtotals.total_cases/(covidtotals.population/1000000),inplace=True)

covidtotals.total_deaths_pm.fillna(covidtotals.total_deaths/ (covidtotals.population/1000000), inplace = True)

covidtotals[totavars].isnull().sum(axis = 0)

covidtotals[demovars].isnull().sum(axis = 0)

covidtotals.loc[totavarmisscnt > 0].T



"""## Identifying outliers with one variable"""

!pip install scipy
!pip install statmodels

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import scipy.stats as scistat

covidtotals = pd.read_csv("covidtotals.csv")
covidtotals.set_index("iso_code", inplace = True)
totavars = ['location', 'total_cases', 'total_deaths', 'total_cases_pm', 'total_deaths_pm']
demovars = ['population', 'pop_density', 'median_age', 'gdp_per_capita', 'hosp_beds']

# get the descriptive statics for the covid case data

covidtotalsonly = covidtotals.loc[:, totavars]
covidtotalsonly.describe()

# show more detailed percentile data

covidtotalsonly.quantile(np.arange(0.0, 1.1, 0.1))

covidtotalsonly.kurtosis()

# test the covid for normality

# Use the Shapiro-Wilk test from the scipy library. Print out the p-value from the test.
# (The null hypothesis of a normal distribution can be rejected at the 95% level at any p-value below 0.05.):

def testnorm(var, df):
  stat, p = scistat.shapiro(df[var])
  print("stat", stat ," p value", p )

testnorm("total_cases", covidtotalsonly)
testnorm("total_deaths", covidtotalsonly)
testnorm("total_cases_pm", covidtotalsonly)
testnorm("total_deaths_pm", covidtotalsonly)

# show normal quantile plots (qqplots) of total cases and total cases per million

# The stright lines show what the distribution would like if they were normal

sm.qqplot(covidtotalsonly[['total_cases']].sort_values(['total_cases']), line = "s")
plt.title("QQ plot of Total cases")
sm.qqplot(covidtotalsonly[['total_cases_pm']].sort_values(['total_cases_pm']), line = "s")
plt.title("QQ plot of tototal case per million")

plt.show()

# even when adjusted by population with the total cases per million column, the distribbution is substantially
# different from normal

# show the outlier range for total cases

thirdq, firstq = covidtotalsonly.total_cases.quantile(0.75), covidtotalsonly.total_cases.quantile(0.25)
interquatilerange = 1.5*(thirdq - firstq)
outlierhigh, outlierlow = interquatilerange + thirdq, firstq - interquatilerange

print(outlierlow, outlierhigh, sep = "<-->")

# Generate a data frame of otlier and write it to excel

def getoutliers():
  dfout = pd.DataFrame(columns = covidtotals.columns, data = None)

  for col in covidtotalsonly.columns[1:]:
      thirdq, firstq = covidtotalsonly[col].quantile(0.75), covidtotalsonly[col].quantile(0.25)
      interquatilerange = 1.5* (thirdq - firstq)
      outlierhigh,outlierlow = thirdq + interquatilerange, firstq- interquatilerange

      df = covidtotals.loc[(covidtotals[col]> outlierhigh) | (covidtotals[col] < outlierlow)]
      df = df.assign(varname = col, threshlow = outlierlow, threshhigh = outlierhigh)
      dfout = pd.concat([dfout, df])

  return dfout


outliers = getoutliers()
outliers.varname.value_counts()

outliers.to_excel("outliercases.xlsx")

# look a little more closely at outliers for cases per million

outliers.loc[outliers.varname == "total_cases_pm", ['location', 'total_cases_pm', 'pop_density', 'gdp_per_capita']].\
            sort_values(['total_cases_pm'], ascending = False)

covidtotals[['pop_density','gdp_per_capita']].quantile([0.25, 0.50, 0.75])

plt.hist(covidtotals[['total_cases']]/1000, bins = 7)
plt.title("Total covid cases (thousands)")
plt.xlabel("Cases")
plt.ylabel("Number of countries")
plt.show()

# perform a log information of the covid data. show a histrogram of the log transformation of total cases

covidlogs = covidtotalsonly.copy()


for col in covidtotalsonly.columns[1:]:
  covidlogs[col] = np.log1p(covidlogs[col])

plt.hist(covidlogs['total_cases'], bins = 7)
plt.title("Total Covid cases")
plt.xlabel("cases")
plt.ylabel("Number of countries")
plt.show()

# The tools we used in the preceding steps tell us a fair bit about how Covid cases and deaths are distributed, and
# about where outliers are located.



"""## *Identifying outliesrs and unexpected values in bivariate reraltionship italicized text*"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

covidtotals = pd.read_csv('covidtotals.csv')
covidtotals.set_index("iso_code", inplace = True)

totvars = ['location', 'total_cases', 'total_deaths','total_cases_pm', 'total_deaths_pm']
demovars = ['population', 'pop_density', 'median_age', 'gdp_per_capita', 'hosp_beds']

# Generate a correlation matrix for the cumulative and  demographic columns

covidtotals.corr(method = "pearson")

# check the see whether some countries have unexpectedly high or low total deaths, given total cases

covidtotalsonly = covidtotals.loc[:, totvars]
covidtotalsonly['total_cases_q'] = pd.qcut(covidtotalsonly['total_cases'], labels = ['very low', 'low', 'medium', 'high', 'very high'],\
                                            q= 5, precision = 0 )
covidtotalsonly['total_deaths_q'] = pd.qcut(covidtotalsonly['total_deaths'], labels = ['very low', 'low', 'medium', 'high', 'very high'], \
                                            q = 5, precision = 0)
pd.crosstab(covidtotalsonly.total_cases_q, covidtotalsonly.total_deaths_q)

# Take a look at countries that do not fit along the diagonal

covidtotals.loc[(covidtotalsonly.total_cases_q == "very high") & (covidtotalsonly.total_deaths_q == "medium") ].T

covidtotals.loc[(covidtotalsonly.total_cases_q == "low") & (covidtotalsonly.total_deaths_q == "high")].T

# Do a scatter plot of total cases by total deaths
# use a regplot method to generate a linear regression line in addition to the scatter plot

ax = sns.regplot (x = "total_cases", y = "total_deaths", data = covidtotals)
ax.set(xlabel = "cases", ylabel = "deaths", title = "Total covid cases and deaths by country")
plt.show()

# Examine the unexected values above the regresion line

covidtotals.loc[(covidtotals.total_cases > 300000) & (covidtotals.total_deaths > 20000)].T

# Do a scatter plot of total cases per million by total deaths per million

ax = sns.regplot(x = "total_cases_pm", y = "total_deaths_pm", data = covidtotals)
ax.set(xlabel = "Cases per million", ylabel = "Deaths per million ", title = " Total covid cases per million and deaths per million by country")
plt.show()

# Examine deaths per million above and below the regression line

covidtotals.loc[(covidtotals.total_cases_pm < 7500 ) & (covidtotals.total_deaths_pm < 250), ['location', 'total_cases_pm', 'total_deaths_pm']]



"""# **Using subsetting to examine logical inconsistencies in variable relationships**"""

import pandas as pd
import numpy as np
nls97 = pd.read_csv("nls97.csv")
nls97.set_index("personid", inplace = True)

nls97[["wageincome", "highestgradecompleted", "highestdegree"]].head(3).T

#We use the ability of the loc accessor to choose all columns
#from the column indicated on the left of the colon through the column indicated on the right; for example,
#nls97.loc[:, "colenroct09":"colenrfeb14"]:

nls97.loc[:, "weeksworked12":"weeksworked17"].head(3).T

nls97.loc[:, "colenroct09":"colenrfeb14"].head(3).T

# show individual with wage income but no weeks worked

nls97.loc[(nls97.weeksworked16 == 0) & (nls97.wageincome > 0), ["weeksworked16", "wageincome"]]

# check wether an individual was ever enrolled in a year college course

nls97.filter(like = "colenr").apply(lambda x : x.str[0:1] == "3").head(3).T

nls97.filter(like = "colenr").apply(lambda x : x.str[0:1] == "3").any(axis = 1).head(2)

# show individuals with post-graduate enrollment but no bachellor's enrollment

nobach = nls97.loc[(nls97.filter(like = "colenr").apply(lambda x : x.str[0:1] == "4").any(axis = 1)) \
                   &~ (nls97.filter(like = "colenr").apply(lambda x : x.str[0:1]== "3").any(axis = 1)) ,"colenrfeb97":"colenroct17"]

len(nobach)

nobach.head(3).T

# show individuals with bacheleors or more , but no 4 year college enrollement

nls97.highestdegree.value_counts(sort = False)

no4yearenrollement = nls97.loc[nls97.highestdegree.str[0:1].isin(['4', '5','6', '7']) &~\
                               nls97.filter(like = "colenr").apply(lambda x : x.str[0:1] == "3").any(axis = 1), "colenrfeb97": "colenroct17"]

len(no4yearenrollement)

no4yearenrollement.head(3).T

# show individuals with high wage income

highwages = nls97.loc[nls97.wageincome > (nls97.wageincome.std()*3)+(nls97.wageincome.mean()), ['wageincome']]
highwages

# show individuals with large changes in weeks worked for the most rescent year

workchanges = nls97.loc[~nls97.loc[:,"weeksworked12":"weeksworked16"].mean(axis = 1).\
                        between(nls97.weeksworked17*0.5, nls97.weeksworked17*2)\
                        & ~nls97.weeksworked17.isnull(), "weeksworked12": "weeksworked17"]

len(workchanges)



"""# **Using linear regression to identify data points with significant influences**"""

import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

covidtotals = pd.read_csv("covidtotals.csv")
covidtotals.set_index("iso_code", inplace = True)

xvars = ['pop_density', 'median_age', 'gdp_per_capita']
covidanalysis = covidtotals.loc[:, ['total_cases_pm'] + xvars].dropna()
covidanalysis.describe()

covidanalysis.isnull().sum()

# fit a linear regression model

def getlm(df):
  y = df.total_cases_pm
  x = df[['pop_density', 'median_age', 'gdp_per_capita']]
  x = sm.add_constant(x)

  return sm.OLS(y, x).fit()


lm = getlm(covidanalysis)
lm.summary()

# identifies those countries with an outsized influence on the model

influence = lm.get_influence().summary_frame()
influence.loc[influence.cooks_d > 0.5, ['cooks_d']]

# cooks's distance values of greater than 0.5 should be scrutinized closely

covidanalysis.loc[influence.cooks_d > 0.5]

# do an influence plot

fig, ax = plt.subplots(figsize = (10,6))
sm.graphics.influence_plot(lm, ax = ax, criterion = "cooks")
plt.show()

ax

# run the model without the two outliers

covidanalysisminusoutliers = covidanalysis.loc[influence.cooks_d <0.5]
lm = getlm(covidanalysisminusoutliers)
lm.summary()

fig, ax = plt.subplots(figsize = (10,6))
sm.graphics.influence_plot(lm, ax = ax, criterion = "cooks")
plt.show()



"""# **Using k-nearest neighbour to find outliers**"""

!pip install pyod

import pandas as pd
from pyod.models.knn import KNN
from sklearn.preprocessing import StandardScaler
covidtotals = pd.read_csv("covidtotals.csv")
covidtotals.set_index("iso_code", inplace = True)

# Create a standardized data frame of the anlysis columns

standardizer = StandardScaler()
analysisvars = ['location', 'total_cases_pm', 'total_deaths_pm','pop_density', 'median_age', 'gdp_per_capita']
covidanalysis = covidtotals.loc[:, analysisvars].dropna()

covidanalysisstand =  standardizer.fit_transform(covidanalysis.iloc[:,1:])

# run the KNN model and generate anomaly scores
# We create an arbitry number of outliers by setting the contamination parameter to 0.1

clf_name = 'KNN'
clf = KNN(contamination=0.1)
clf.fit(covidanalysisstand)

y_pred = clf.labels_
y_scores = clf.decision_scores_

# show the prediction from the model

pred = pd.DataFrame(zip(y_pred, y_scores), columns = ['outlier', 'scores'], index= covidanalysis.index)
pred.sample(10, random_state = 1)

#Notice that the decision scores for outliers are all higher than those for the inliers (outlier = 0):

pred.outlier.value_counts()

# show the covid data for the outliers

# 01 first merge the covidanalysis and pred data frames

covidanalysis.join(pred).loc[pred.outlier == 1, ['location', 'total_cases_pm', 'total_deaths_pm', 'scores']].sort_values(['scores'], ascending = False)

covidanalysis.join(pred).loc[pred.outlier == 0, ['location', 'total_cases_pm', 'total_deaths_pm', 'scores']].sort_values(['scores'], ascending = False)



"""# **Using Isolation forest to find anomalies**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from mpl_toolkits.mplot3d import Axes3D

covidtotals = pd.read_csv("covidtotals.csv")
covidtotals.set_index("iso_code", inplace = True)

# Create a standardized analysis data Frame

analysisvars = ['location', 'total_cases_pm', 'total_deaths_pm', 'pop_density', 'median_age', 'gdp_per_capita']
standardizer = StandardScaler()
covidtotals.isnull().sum()

covidanalysis = covidtotals.loc[:, analysisvars].dropna()
covidanalysisstand = standardizer.fit_transform(covidanalysis.iloc[:,1:])

# run the isolation forest model to detect outliers

# Pass the standardized data to the fit method. 18 countries are identified as outliers. (These countries have
# anomaly values of -1.) This is determined by the contamination number of 0.1:

clf = IsolationForest(n_estimators = 100, max_samples = 'auto', contamination = 0.1, max_features = 1.0)
clf.fit(covidanalysisstand)

covidanalysis['anomaly'] = clf.predict(covidanalysisstand)
covidanalysis['scores'] = clf.decision_function(covidanalysisstand)
covidanalysis.anomaly.value_counts()

# create outlier and inlier Data frames

inlier, outlier =  covidanalysis.loc[covidanalysis.anomaly == 1], covidanalysis.loc[covidanalysis.anomaly == -1]
outlier[['location', 'total_cases_pm','total_deaths_pm', 'median_age', 'gdp_per_capita', 'scores']].sort_values(['scores']).head(10)

# plot the outliers and inliers

ax = plt.axes(projection = '3d')
ax.set_title('Isolocation Forest Anomaly Detection')
ax.set_zlabel("Cases Per Million")
ax.set_ylabel("Median Age")
ax.set_xlabel("GDP Per capita")
ax.scatter3D(inlier.gdp_per_capita, inlier.median_age, inlier.total_cases_pm, label = "inliers", c = "blue")
ax.scatter3D(outlier.gdp_per_capita, outlier.median_age, outlier.total_cases_pm, label = "outliers", c = "red")
ax.legend()
plt.tight_layout()
plt.show()



"""# ***Chapter 05 :***

# **Using viqualizations for the identification of unexpected values**

## **Using histograms to examine the distribution of continuous variables**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
landtemps = pd.read_csv("landtemps2019avgs.csv")
covidtotals = pd.read_csv('covidtotals.csv', parse_dates = ['lastdate'])
covidtotals.set_index('iso_code', inplace = True)

# shoe some of the station temperature  rows

landtemps[['station', 'country', 'latabs', 'elevation', 'avgtemp']].sample(10, random_state = 1)

# show the descriptive statistics

landtemps.describe()

landtemps.avgtemp.skew()

landtemps.avgtemp.kurtosis()

# Do a histogram of average temperature

plt.hist(landtemps.avgtemp)
plt.axvline(landtemps.avgtemp.mean(), color = 'red', linestyle = 'dashed', linewidth = 1)
plt.title("Histogram of Average Temperatures (celsius)")
plt.xlabel("Average Temperature")
plt.ylabel("Frequency")
plt.show()

# run qq plot to examine where the distribution deviates from a normal distribution

sm.qqplot(landtemps[['avgtemp']].sort_values(['avgtemp']), line = 's')
plt.title("QQ plot of average Temperature")
plt.show()

# show the skewness and kurtosis for covid cases per million

covidtotals.total_cases_pm.skew()

covidtotals[['total_cases_pm']].skew()

covidtotals.total_cases_pm.kurtosis()

# do a stacked histogram of the covid case data

showregions = ['Oceania / Aus', 'East Asia', 'Southern Africa', 'Western Europe']

def getcases(regiondesc):
  return covidtotals.loc[covidtotals.region == regiondesc  , 'total_cases_pm']

plt.hist([getcases(k) for k in showregions], color = ['blue', 'mediumslateblue', 'plum', 'mediumvioletred'], label = showregions, stacked  = True)

plt.title("Stacked Histogram of cases per million for Selected Regions")
plt.xlabel("Cases Per Million")
plt.ylabel("Frequency")
plt.xticks(np.arange(0, 22500, step= 2500))
plt.legend()
plt.show()

# show multiple histogram on one figure

fig , axes = plt.subplots(2,2)
fig.suptitle("Histogram of Covid cases per Million by selected Regions")
axes = axes.ravel()

for j, ax in enumerate(axes):
  ax.hist(covidtotals.loc[covidtotals.region == showregions[j]].total_cases_pm, bins = 5)
  ax.set_title(showregions[j], fontsize = 10)
  for tick in ax.get_xticklabels():
    tick.set_rotation(45)

plt.tight_layout()
fig.subplots_adjust(top= 0.88)
plt.show()



"""## **Using boxplots to identify outliers for continuous variables**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

covidtotals = pd.read_csv("covidtotals.csv", parse_dates = ["lastdate"])
nls97 = pd.read_csv("nls97.csv")
nls97.set_index("personid", inplace = True)
covidtotals.set_index("iso_code", inplace = True)

# Do a boxplot of SAT verbal scores

nls97.satverbal.describe()

plt.boxplot(nls97.satverbal.dropna(), labels = ['SAT VERBAL'])
plt.annotate("outlier threshold", xy = (1.05,780), xytext = (1.15, 780), size = 7,\
             arrowprops = dict(facecolor = 'black', headwidth = 2, width = 0.5, shrink = 0.02))

plt.annotate("3rd quartile", xy = (1.08,570), xytext = (1.15,570), size = 7, \
             arrowprops = dict(facecolor= 'black',headwidth  = 2, width = 0.5, shrink = 0.02))

plt.annotate("median", xy = (1.08,500), xytext = (1.15,500), size = 7, \
             arrowprops = dict(facecolor= 'black',headwidth  = 2, width = 0.5, shrink = 0.02))

plt.annotate("1st quartile", xy = (1.08,430), xytext = (1.15,430), size = 7, \
             arrowprops = dict(facecolor= 'black',headwidth  = 2, width = 0.5, shrink = 0.02))

plt.annotate("outlier thershold", xy = (1.05,220), xytext = (1.15,220), size = 7, \
             arrowprops = dict(facecolor= 'black',headwidth  = 2, width = 0.5, shrink = 0.02))

# show some descriptive on weeks worked
weeksworked = nls97.loc[:, ['highestdegree', 'weeksworked16','weeksworked17']]
weeksworked.describe()

plt.boxplot([weeksworked.weeksworked16.dropna(), weeksworked.weeksworked17.dropna()],\
            labels = ['Weeks worked 2016','Weeks worked 2017'])
plt.title("Boxplot of weeks  worked")
plt.tight_layout()
plt.show()



"""# **Using Grouped boxplots to uncover unexpected values in a particular group**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


nls97 = pd.read_csv("nls97.csv")
nls97.set_index("personid", inplace = True)

covidtotals = pd.read_csv("covidtotals.csv")
covidtotals.set_index("iso_code", inplace = True)

# do a violin plot of the SAT verbal score

sns.violinplot(nls97['setvern'], color="wheat", orient = "V")
plt.title("violin plot of SAT verbal score")
plt.ylabel("SAT verbal")
plt.text(0.08, 780, "outlier thershold", horizontalalignment = "center", size = 8)
plt.text(0.065, nls97.satverbal.quantile(0.75) ,"3rd quartile", horizontalalignment = "center", size = 8)
plt.text(0.05, nls97.satverbal.median(),"median", horizontalalignment = "center", size = 8)
plt.text(0.065, nls97.satverbal.quantile(0.25) , "1st quartile", horizontalalignment = "center", size = 8)
plt.text(0.08, 210, "outlier threshold", horizontalalignment = "center", size = 8)
plt.text(-0.4, 500, "frequency", horizontalalignment = "center", size = 8)
plt.show()

# Get more descriptive for weeks worked
nls97.loc[:, ['weeksworked16','weeksworked17' ]].describe()

# show weeksworked for 2016 and 2017

myplt = sns.violinplot(data = nls97.loc[:, ['weeksworked16', 'weeksworked17']])
myplt.set_title("Vilolin plots of weeks worked ")
myplt.set_xticklabels(['weeks worked 2016', 'weeks worked 2017'])
plt.show()

# Do a violin plot of wage income by gender and marital status

nls97["maritalstatuscollapsed"] = nls97.maritalstatus.replace(['Married', 'Never-married', 'Divorced', 'Separated', 'Widowed'], ['Married', 'Never Married', 'Not Married', 'Not Married', 'Not Married'])
sns.violinplot(nls97.gender, nls97.wageincome, hue = nls97.maritalstatuscollapsed, scale = "count")
plt.title("Violin Plots of wage income by gender and Marital Status")
plt.xlabel('Gender')
plt.legend(title = "", loc = "upper center", framealpha = 0, fontsize = 8)
plt.tight_layout()
plt.show()

# do a violin plots of weeks worked by highest degree attained

myplt = sns.violinplot('highestdegree', 'weeksworked17', data = nls97, rotation = 40)
myplt.set_xticklabels(myplt.get_xticklabels(), rotation = 60, horizontalalignment = 'right')
myplt.set_title("Violin plot of weeks worked by Highest Degree")
myplt.set_xlabel("Highest degree attained")
myplt.set_ylabel("Weeks Worked 2017")
plt.tight_layout()
plt.show()

"""# Using scatter plots to view bivariate relationship"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

landtemps = pd.read_csv("landtemps2019avgs.csv")

# run a scatterplot of lattitude (latabs ) by average temperature

plt.scatter(x = "latabs", y = "avgtemp", data = landtemps)
plt.xlabel("Latitude (N or S)")
plt.ylabel("Average Temperature (celsius)")
plt.yticks(np.arange(-60,40, step = 20))
plt.title("Lattitude and Average Temperature in 2019")
plt.show()

# show high elevation points in red

low, high = landtemps.loc[landtemps.elevation <= 1000 ],landtemps.loc[landtemps.elevation > 1000]
plt.scatter(x = "latabs", y = "avgtemp", c = "blue", data = low)
plt.scatter(x = "latabs", y = "avgtemp", c = "red", data = high)
plt.legend(["Low Elevation", "High Elevation"])
plt.xlabel("Lattitude (N or S)")
plt.ylabel("Average Temperature (celsius)")
plt.title("Latitude and Average temperature in 2019")
plt.show()

# view a three dimensional plot of temperature latitude and elevation
fig = plt.figure()
plt.suptitle("Lattitude , Temperature, and Elavation in 2019")
ax = plt.axes(projection = '3d')
ax.set_title("Three D")
ax.set_xlabel("Elevation")
ax.set_ylabel("Latitude")
ax.set_zlabel("Avg temp")
ax.scatter3D(low.elevation, low.latabs, low.avgtemp, label = "low Elevation", c = "blue")
ax.scatter3D(high.elevation, high.latabs, high.avgtemp, label = "High Elevation", c = "red")
ax.legend()
plt.show()

# show a regression line of lattitude  on the temperature data

sns.regplot(x = "latabs", y = "avgtemp", data = landtemps, color = "blue")
plt.title("Latitude and Average Temperature in 2019")
plt.xlabel("Latitude (N or S)")
plt.ylabel("Average Temperature")
plt.show()

# show seperate regression lines for low and high elevation stations

landtemps["elevation_groups"] = np.where(landtemps.elevation <= 1000 , 'low', 'high')
sns.lmplot(x = "latabs", y = "avgtemp", hue = "elevation_groups", palette = dict(low = "blue", high = "red"),\
           legend_out = False, data = landtemps)
plt.xlabel("Latitude (N or S)")
plt.ylabel("Average temperature")
plt.legend(('low elevation', 'high elevation'), loc = "lower left")
plt.yticks(np.arange(-60, 40, step = 20))
plt.title ("Latitude and Average Temperature in 2019")
plt.tight_layout()
plt.show()

# show some stations above the low and high elevation lines

high.loc[(high.latabs > 38) & (high.avgtemp >= 18), ['station', 'country', 'latabs', 'elevation', 'avgtemp']]

low.loc[(low.latabs > 47) & (low.avgtemp >= 14), ['station','country', 'latabs', 'elevation', 'avgtemp']]

#show some stations below the low and high elevation regression line

high.loc[(high.latabs < 5) & (high.avgtemp < 18), ['station', 'country', 'latabs', 'elevation', 'avgtemp']]



"""# **Using line plots to examine trends in continuous variables**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter
coviddaily = pd.read_csv("coviddaily720.csv", parse_dates = ['casedate'])

# view couple of rows of the covid daily data
coviddaily.sample(2, random_state = 1).T

# calculate new cases and deaths by day

# Select dates between 2020-02-01 and 2020-07-12, and then use groupby to summarize cases and
# deaths across all countries for each day:

coviddailytotals = coviddaily.loc[coviddaily.casedate.between('2020-02-01', '2020-07-12')].\
                    groupby(['casedate'])[['new_cases', 'new_deaths']].sum().reset_index()
coviddailytotals.sample(7, random_state = 1)

# 04. show line plot for new cases and new deaths by day
fig = plt.figure()
fig.suptitle("New Covid Cases and Deaths by Day Worldwide in 2020")
ax1 = plt.subplot(2,1,1)
ax1.plot(coviddailytotals.casedate, coviddailytotals.new_cases)
ax1.xaxis.set_major_formatter(DateFormatter("%b"))
ax1.set_xlabel("New Cases")

ax2 = plt.subplot(2,1,2)
ax2.plot(coviddailytotals.casedate, coviddailytotals.new_deaths)
ax2.xaxis.set_major_formatter(DateFormatter("%b"))
ax2.set_xlabel("New Deaths")

plt.tight_layout()
fig.subplots_adjust(top = 0.80)
plt.show()

# Calculate new cases ans deaths by day and region

regiontotals = coviddaily.loc[coviddaily.casedate.between('2020-02-01', '2020-07-12')]\
              .groupby(['casedate', 'region'])[['new_cases','new_deaths']].sum().reset_index()
regiontotals

regiontotals.sample(7, random_state = 1)

# show the line plots of new cases by selected regions

showregions = ['East Asia', 'Southern Africa', 'North America', 'Western Europe']
for j in range(len(showregions)):
  rt = regiontotals.loc[regiontotals.region == showregions[j],['casedate', 'new_cases']]
  plt.plot(rt.casedate, rt.new_cases, label = showregions[j])

plt.title("New covid cases by Day and Region in 2020")
plt.gca().get_xaxis().set_major_formatter(DateFormatter("%b"))
plt.ylabel("New Cases")
plt.legend()
plt.show()

# use to stack plot to examine the uptick in sounthern africa more  closely

af = regiontotals.loc[regiontotals.region == 'Southern Africa', ['casedate', 'new_cases']].\
      rename(columns = {'new_cases': 'afcases'})
sa = coviddaily.loc[coviddaily.location == "South Africa", ['casedate', 'new_cases']].rename(columns = {'new_cases':'sacases'})

af = pd.merge(af, sa, left_on = ['casedate'], right_on = ['casedate'], how = "left")
af.sacases.fillna(0, inplace = True)
af['afcasesnosa'] = af.afcases - af.sacases

afabb = af.loc[af.casedate.between('2020-04-01', '2020-07-12')]
fig = plt.figure()
ax = plt.subplot()
ax.stackplot(afabb.casedate, afabb.sacases, afabb.afcasesnosa, labels = ['South Africa', 'Other Southern Africa'])

ax.xaxis.set_major_formatter(DateFormatter("%m-%d"))
plt.title("New covid cases in southern africa")
plt.tight_layout()
plt.legend(loc = "upper left")
plt.show()



"""# **Genearating a heat map based on a correlation matrix**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

covidtotals = pd.read_csv("covidtotals.csv", parse_dates = ['lastdate'])

# Generate correlation matrix

corr = covidtotals.corr()

corr[['total_cases', 'total_deaths', 'total_cases_pm', 'total_deaths_pm']]

# show scatter plot of median age and gross deomestic product per capita by cases per million

fig, axes = plt.subplots(1,2, sharey = True)
sns.regplot(covidtotals.median_age, covidtotals.total_cases_pm, ax = axes[0])
sns.regplot(covidtotals.gdp_per_capita, covidtotals.total_cases_pm, ax = axes[1])
axes[0].set_xlabel("Median Age")
axes[0].set_ylabel("Cases per Million ")
axes[1].set_xlabel("Gdp per capita")
axes[1].set_ylabel("")
plt.suptitle("Scatter plots of age GDP with cases per Million")
plt.tight_layout()
fig.subplots_adjust( top =  0.92)
plt.show()

# Generate heat map of the correlation matrix

sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, cmap = "coolwarm", annot = True)
plt.title("Heat map Correlation Matrix")
plt.tight_layout()
plt.show()



"""<h1 align = "center"><b>Chapter 02</b></h1>

# **06.Cleaning and Exploring Data with Series Operations**

# **Getting value form a pandas series**
"""

import pandas as pd
nls97 = pd.read_csv("nls97b.csv")
nls97.set_index("personid", inplace = True)

# create a series from the GPA overall column
gpaoverall = nls97.gpaoverall
type(gpaoverall)

gpaoverall.head()

gpaoverall.index

# select GPA values using the bracket operatior

gpaoverall[:5]

# select values using loc accessor
gpaoverall.loc[100061]

gpaoverall.loc[[100061]]

gpaoverall.loc[[100061,100139, 100284]]

gpaoverall.loc[100061 :100833]

# select the values using the iloc accessor

# iloc differs from loc in that it takes a list of row numbers rather than labels. It works similarly to
# bracket operator slicing. In this step, we pass a one-item list with the value of 0. We then pass a five-item
# list, [0,1,2,3,4], to return a series containing the first five values. We get the same result if we
# pass [:5] to the accessor:

gpaoverall.iloc[[0]]

gpaoverall.iloc[0]

gpaoverall.iloc[[0,1,2,3]]

gpaoverall.iloc[:5]

"""# **Showing summary Statistics for a pandas series**"""

import pandas as pd
import numpy as np
nls97 = pd.read_csv("nls97b.csv")
nls97.set_index("personid", inplace = True)

gpaoverall = nls97.gpaoverall

gpaoverall.mean()

gpaoverall.describe()

gpaoverall.quantile(np.arange(0.1, 1.1, 0.1))

# show descriptive for a subset of the series

gpaoverall.loc[gpaoverall.between(3,3.5)].head()

gpaoverall.loc[(gpaoverall < 2) | (gpaoverall > 4)].sample(5, random_state = 2)

gpaoverall.loc[(gpaoverall > gpaoverall.quantile(0.99))].agg(['count', 'min', 'max'])

# test for the a condition across all values

(gpaoverall > 4).any() # any person has GPA greater than 4

(gpaoverall >=0).all() # all people have gpa grater than or equal to 0

(gpaoverall >= 0).sum()  # of the people with gpa grater than or equal 0

gpaoverall.isnull().sum()

# show descriptive for a subset of the series based on values in a different column

nls97.loc[(nls97.wageincome > nls97.wageincome.quantile(0.75)),'gpaoverall'].mean()

nls97.loc[(nls97.wageincome < nls97.wageincome.quantile(0.25)), 'gpaoverall'].mean()

# show descriptives and frequencies for a series cotaining categorical data

nls97.maritalstatus.describe()

nls97.maritalstatus.value_counts()



"""# **Changing Series values**

During the data cleaning process, we often need to change the values in a data series or create a new one.
We can change all the values in a series, or just the values in a subset of our data. Most of the techniques we
have been using to get values from a series can be used to update series values, though some minor
modifications are necessary
"""

import pandas as pd
nls97 = pd.read_csv('nls97b.csv')
nls97.set_index("personid", inplace = True)

# Edit all the values based on a scaler
# multiply the values based on scaler

nls97.gpaoverall.head()

gpaoverall100 = nls97['gpaoverall']*100
gpaoverall100.head()

# Set values using index labels

nls97.loc[[100061], 'gpaoverall'] = 3

nls97.loc[[100139, 100284, 100292], 'gpaoverall'] = 0
nls97.gpaoverall.head()

# set values using an operator on more than one series

nls97['childnum'] = nls97.childathome + nls97.childnotathome
nls97.childnum.value_counts().sort_index()

# set the values for a summury statistic using index labels

nls97.loc[100061:100292, 'gpaoverall'] = nls97.gpaoverall.mean()

nls97.gpaoverall.head()

# set the value using position
# use the iloc operator to select the position

nls97.iloc[0,13] = 2
nls97.iloc[1:4, 13] = 14

nls97.gpaoverall.head()

# set GPA value after filtering
nls97.gpaoverall.nlargest()

nls97.loc[nls97.gpaoverall > 4, 'gpaoverall'] = 4

nls97.gpaoverall.nlargest()



"""# **Changing Series Values Conditionally**"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv("nls97b.csv")
nls97.set_index("personid", inplace = True)
landtemps = pd.read_csv("landtemps2019avgs.csv")

# use numpy's where function to create categorical series containing two values

landtemps.elevation.quantile(np.arange(0.2,1.1, 0.2))

landtemps['elevation_group'] = np.where(landtemps.elevation > landtemps.elevation.quantile(0.8),'High', 'Low')
landtemps.elevation_group = landtemps.elevation_group.astype('category')
landtemps.groupby(['elevation_group'])['elevation'].agg(['count', 'min', 'max'])

landtemps.elevation_group.value_counts()

# use numpy's Select method to evaluate a list of conditions

test = [(nls97.gpaoverall < 2 ) & (nls97.highestdegree == '0. None'),\
        nls97.highestdegree == '0. None', nls97.gpaoverall < 2 ]
result = ['1. Low GPA and No Diploma', '2. No Diploma', '3. Low GPA']

nls97['hsachieve'] = np.select(test, result, '4. Did Okay')
nls97[['hsachieve', 'gpaoverall', 'highestdegree']].head()

nls97.hsachieve.value_counts().sort_index()

# use lambda to test several column in one statement

nls97.loc[[100292, 100583,100139], 'colenrfeb00':'colenroct04'].T

nls97['baenrollment'] = nls97.filter(like = 'colenr').apply(lambda x : x.str[0:1] == '3').any(axis = 1)
nls97.loc[[100292, 100583, 100139], ['baenrollment']].T

nls97.baenrollment.value_counts()

# create a function that assigns a value based on the value of several series

def getsleepdeprivedreason(row):
  sleepdeprivedreason = "Unknown"
  if (row.nightlyhrssleep >= 6):
    sleepdeprivedreason = "Not Sleep Deprived"
  elif(row.nightlyhrssleep > 0):
    if(row.weeksworked16 + row.weeksworked17 < 80):
      if (row.childathome > 2):
        sleepdeprivedreason = "Child Rearing"
      else:
        sleepdeprivedreason = "Other Reason"
    else:
      if(row.wageincome >= 6200 or row.highestgradecompleted >=16):
        sleepdeprivedreason = "Work Pressure"
      else:
        sleepdeprivedreason = "Income Pressure"
  else:
      sleepdeprivedreason = "Unknown"

  return sleepdeprivedreason

# use applyt to run the fucntion for all rows

nls97['sleepdeprivedreason'] = nls97.apply(getsleepdeprivedreason, axis = 1)
nls97.sleepdeprivedreason = nls97.sleepdeprivedreason.astype('category')
nls97.sleepdeprivedreason.value_counts()

def getsleepdeprivedreason(childathome, nightlyhrssleep, wageincome, weeksworked16, weeksworked17, highestgradecompleted):
  sleepdeprivedreason = "Unknown"
  if (nightlyhrssleep >= 6):
    sleepdeprivedreason = "Not Sleep Deprived"
  elif(nightlyhrssleep > 0):
    if(weeksworked16 + weeksworked17 < 80):
      if (childathome > 2):
        sleepdeprivedreason = "Child Rearing"
      else:
        sleepdeprivedreason = "Other Reason"
    else:
      if(wageincome >= 6200 or highestgradecompleted >=16):
        sleepdeprivedreason = "Work Pressure"
      else:
        sleepdeprivedreason = "Income Pressure"
  else:
      sleepdeprivedreason = "Unknown"

  return sleepdeprivedreason

nls97['sleepdeprivedreasoninlamda'] = nls97.apply(lambda x:\
                                                  getsleepdeprivedreason(x.childathome,x.nightlyhrssleep,\
                                                                         x.wageincome, x.weeksworked16,\
                                                                         x.weeksworked17, x.highestgradecompleted), axis = 1)
nls97.sleepdeprivedreasoninlamda = nls97.sleepdeprivedreasoninlamda.astype('category')
nls97.sleepdeprivedreasoninlamda.value_counts()



"""# **Evaluating and cleaning string series data**"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv("nls97c.csv")
nls97.set_index("personid", inplace = True)

nls97.govprovidejobs.value_counts()

nls97['govprovidejobsdefprob'] = np.where(nls97.govprovidejobs.isnull(), np.nan, np.where(nls97.govprovidejobs.str.contains("not"), "No", "Yes"))
pd.crosstab(nls97.govprovidejobs, nls97.govprovidejobsdefprob)

nls97.govprovidejobs.isnull().value_counts()

# Handle leading or trailing spaces in a string

nls97.maritalstatus.value_counts()

nls97.maritalstatus.str.startswith(' ').any()

nls97.maritalstatus.str.endswith(' ').any()

nls97['evermarried'] = np.where(nls97.maritalstatus.isnull(), np.nan, np.where(nls97.maritalstatus.str.strip()== "Never-married", 'No', 'Yes'))
pd.crosstab(nls97.maritalstatus, nls97.evermarried)

# use isin to compare a string value to a list of values

nls97['receivedba'] = np.where(nls97.highestdegree.isnull(), np.nan,np.where(nls97.highestdegree.str[0:1].isin(['4','5','6','7']), 'Yes', 'No'))
pd.crosstab(nls97.highestdegree, nls97.receivedba)

# use findall to extract numeric values from a text string

# Use findall to create a list of all numbers in the weeklyhrstv (hours spent each week
# watching television) string. The "\d+" regular expression that's passed to findall indicates
# that we just want numbers:

pd.concat([nls97.weeklyhrstv.head(), nls97.weeklyhrstv.str.findall("\d+").head()], axis = 1)

# use the list created by findall() to create a numeric series from the weeklyhrstv text
def getnum(numlist):
  highval = 0
  if(type(numlist) is list):
    lastval = int(numlist[-1])
    if (numlist[0]=='40'):
      highval = 45
    elif (lastval == 2):
      highval = 1
    else:
      highval = lastval -5
  else:
    highval = np.nan
  return highval


nls97['weeklyhrstvnum'] = nls97.weeklyhrstv.str.findall("\d+").apply(getnum)

pd.crosstab(nls97.weeklyhrstv, nls97.weeklyhrstvnum)

# replace the values in a series with alternative values

comphrsold = ['None', 'Less than 1 hour a week', '1 to 3 hours a week', '4 to 6 hours a week',\
              '7 to 9 hours a week', '10 hours or more a week']
comphrsnew = ['A. None', 'B. Less than 1 hours a week', 'C. 1 to 3 hours a week', 'D. 4 to 6 hours a week',\
              'E. 7 to 9 hours a week', 'F. 10 hours or more week']

nls97.weeklyhrscomputer.value_counts().sort_index()

nls97.weeklyhrscomputer.replace(comphrsold, comphrsnew, inplace = True)

nls97.weeklyhrscomputer.value_counts().sort_index()

"""# **Working with dates..**"""

# in this recipe we will convert numeric data in to datatime data

import pandas as pd
import numpy as np
from datetime import datetime

covidcases = pd.read_csv("covidcases720.csv")
nls97 = pd.read_csv("nls97c.csv")
nls97.set_index("personid", inplace = True)

# show the birth month and birth year values

nls97[['birthmonth', 'birthyear']].isnull().sum()

nls97.birthmonth.value_counts().sort_index()

nls97.birthyear.value_counts().sort_index()

# Use the series fillna method to set a value for the missing birth month

nls97.birthmonth.fillna(int(nls97.birthmonth.mean()), inplace = True)
nls97.birthmonth.value_counts().sort_index()

nls97.birthmonth.isnull().sum()

nls97['birthdate'] = pd.to_datetime(dict(year = nls97.birthyear, month = nls97.birthmonth, day = 15))
nls97[['birthyear', 'birthmonth','birthdate']].head()

nls97[['birthmonth', 'birthyear','birthdate']].isnull().sum()

# calculate age values using a datetime column

def calage(startdate, enddate):
  age = enddate.year - startdate.year
  if (enddate.month < startdate.month  or (enddate.month == startdate.month and enddate.day < startdate.day )):
    age = age - 1

  return age

rundate = pd.to_datetime('2020-07-20')
nls97['age'] = nls97.apply(lambda x : calage(x.birthdate, rundate), axis = 1)
nls97.loc[10061:100583, ['age', 'birthdate']]

# Convert string column into a datetime column
# The casedata column is an object data type, not a datetime datatype

covidcases.iloc[:, 0:6].dtypes

covidcases.casedate

covidcases['casedate'] = pd.to_datetime(covidcases.casedate, format = '%Y-%m-%d')
covidcases.iloc[:,0:6].dtypes

# show decriptive statistics on the datetime column

covidcases.casedate.describe()

# create a timedelta object to capture a date interval

firstcase = covidcases.loc[covidcases.new_cases > 0, ['location', 'casedate']].\
            sort_values(['location', 'casedate']).drop_duplicates(['location'], keep = "first").rename(columns = {'casedate':'firstcasedate'})

covidcases = pd.merge(covidcases, firstcase, left_on = ['location'], right_on = ['location'], how= "left" )

covidcases['dayssincefirstcase'] = covidcases.casedate - covidcases.firstcasedate
covidcases.dayssincefirstcase.describe()



"""# **Identifying and cleaning missing data**"""

import pandas as pd
nls97 = pd.read_csv("nls97c.csv")
nls97.set_index("personid", inplace = True)

# set up school record and demographic DataFrame fron the NLS data

schoolrecordlist = ['satverbal', 'satmath', 'gpaoverall', 'gpaenglish', 'gpamath', 'gpascience', 'highestdegree', 'highestgradecompleted']
demolist = ['maritalstatus', 'childathome', 'childnotathome', 'wageincome', 'weeklyhrscomputer', 'weeklyhrstv','nightlyhrssleep']

schoolrecord = nls97[schoolrecordlist]
demo = nls97[demolist]

schoolrecord.shape

demo.shape

# check data for missing values

schoolrecord.isnull().sum(axis = 0)

misscnt = schoolrecord.isnull().sum(axis = 1)
misscnt.value_counts().sort_index()

schoolrecord.loc[misscnt >= 7].head(4).T

# Remove rows where nearly all the data is missing

# Here, we use the dropna DataFrame method with thresh set to 2. This removes rows with
# less than two non-missing values (those with seven or eight missing values)

schoolrecord = schoolrecord.dropna(thresh = 2)
schoolrecord.shape

schoolrecord.isnull().sum(axis = 1).value_counts().sort_index()

# Assign the mean of the GPA values where it's missing

int(schoolrecord.gpaoverall.mean())

schoolrecord.gpaoverall.isnull().sum()

schoolrecord.gpaoverall.fillna(int(schoolrecord.gpaoverall.mean()), inplace = True)
schoolrecord.gpaoverall.isnull().sum()

# Use forward fill to replace missing values

# Use the ffill option with fillna to replace missing values with the nearest non-missing value
# preceding it in the data:

demo.wageincome.head().T

nls97.wageincome.fillna(method = 'ffill', inplace = True)
demo = nls97[demolist]
demo.wageincome.head().T

# Fill missing values with the mean by group

# Create a DataFrame containing the average value of weeks worked in 2017 by the highest degree they've
# earned. Merge that with the NLS data, then use fillna to replace the missing values for weeks
# worked with the mean for that individual's highest degree earned group:

nls97[['highestdegree', 'weeksworked17']].head()

worksbydegree = nls97.groupby(['highestdegree'])['weeksworked17'].mean().reset_index().rename(columns= {'weeksworked17':'meansworked17'})

nls97 = nls97.reset_index().merge(worksbydegree, left_on = ['highestdegree'],\
                                  right_on = ['highestdegree'],how= 'left').set_index('personid')

nls97.weeksworked17.fillna(nls97.meansworked17, inplace = True)

nls97[['highestdegree', 'weeksworked17', 'meansworked17']].head()



"""# **Missing value imputation with K-nearest neighbor**"""

import pandas as pd
from sklearn.impute import KNNImputer
nls97 = pd.read_csv("nls97c.csv")
nls97.set_index("personid", inplace = True)

# select the NLS school record data

schoolrecordlist = ['satverbal', 'satmath', 'gpaoverall', 'gpaenglish', 'gpamath', 'gpascience','highestgradecompleted']
schoolrecord = nls97[schoolrecordlist]

# initialize a KNN imputation model and fill in the values
impKNN = KNNImputer(n_neighbors = 5)
newvalues = impKNN.fit_transform(schoolrecord)
schoolrecordimp = pd.DataFrame(newvalues, columns = schoolrecordlist, index = schoolrecord.index)
schoolrecordimp

# view the imputed values

schoolrecordimp.head().T

schoolrecord.head().T

# compare the summary statistics

schoolrecord[['gpaoverall', 'highestgradecompleted']].agg(['mean', 'count'])

schoolrecordimp[['gpaoverall', 'highestgradecompleted ']].agg(['mean', 'count'])



"""<h1 align= "center"><b>Chapter 07</b></h2>

# **07. Fixing Messy Date When Aggregating**

# *Looping thorough data with itertuples (an anti-pattern)*
"""

import pandas as pd
import numpy as np

coviddaily = pd.read_csv("coviddaily720.csv", parse_dates = ['casedate'])
ltbrazil = pd.read_csv("ltbrazil.csv")

# sort data by location and date

coviddaily = coviddaily.sort_values(['location', 'casedate'])

# iterate over rows with itertuples

# Use itertuples, which allows us to iterate over all rows as named tuples. Sum new cases over
# all dates for each country. With each change of country (location) append the running total to
# rowlist, and then set the count to 0: (Note that rowlist is a list and we are appending a
# dictionary to rowlist with each change of country. A list of dictionaries is a good place to temporarily
# store data you might eventually want to convert to a DataFrame.)

prevloc = 'ZZZ'
rowlist = []


for row in coviddaily.itertuples():
  if (prevloc != row.location):
    if (prevloc != 'ZZZ'):
      rowlist.append({'location': prevloc, 'casecnt': casecnt})
    casecnt = 0
    prevloc = row.location
  casecnt += row.new_cases

#rowlist.append({'location': prevloc, 'casecnt':casecnt})
len(rowlist)

rowlist[0:4]

# create a dataframe from the list of summury values rowlist

# pass the list we created in the previous step to the pandas Dateframe method

covidtotals = pd.DataFrame(rowlist)
covidtotals.head()

#sort the landtemperture data

# also drop rows with missing values for temperature

ltbrazil = ltbrazil.sort_values(['station', 'month'])
ltbrazil = ltbrazil.dropna(subset = ['temperature'])

ltbrazil

# Exclude rows where there is a large change  from one period to the next

# Calculate the average temperature for the year, excluding  values for a temperature more than 3°C greate than\
# or less than the temperature for the previous month

prevstation = 'ZZZ'
prevtemp = 0
rowlist = []

for row in ltbrazil.itertuples():
  if (prevstation != row.station):
    if (prevstation != "ZZZ"):
      rowlist.append({'station':prevstation, 'avgtemp':tempcnt/stationcnt, 'stationcnt':stationcnt})
    tempcnt = 0
    stationcnt = 0
    prevstation = row.station

  # choose only rows that are within 3 degrees of the previous tepmperature

  if((0 <= abs(row.temperature - prevtemp)<= 3) or (stationcnt == 0)):
    tempcnt += row.temperature
    stationcnt += 1
  prevtemp = row.temperature

rowlist.append({'station':prevstation,'avgtemp':tempcnt/stationcnt, 'stationcnt':stationcnt})
rowlist[0:5]

# Create a Dataframe from the summary values

ltbrazilavgs = pd.DataFrame(rowlist)
ltbrazilavgs.head()

"""# **Calculating summaries by group with numpy arrays**"""

import pandas as pd
import numpy as np

coviddaily = pd.read_csv('coviddaily720.csv',parse_dates = ['casedate'])
ltbrazil = pd.read_csv('ltbrazil.csv')

# creation a list of locations
loclist = coviddaily.location.unique().tolist()
loclist

# use a numpy array to calculate sums by location

#   Create a NumPy array of the location and new cases data. We then can iterate over the location list we
# created in the previous step, and select all new case values (casevalues[j][1]) for each
# location (casevalues[j][0]). We then sum the new case values for that location:

rowlist = []
casevalues = coviddaily[['location', 'new_cases']].to_numpy()

for locitem in loclist:
  cases = [casevalues[j][1] for j in range(len(casevalues)) if casevalues[j][0] == locitem]
  rowlist.append(sum(cases))

casevalues

len(rowlist)

len(loclist)

rowlist[0:5]

casetotals = pd.DataFrame(zip(loclist, rowlist), columns = (['location', 'casetotals']))
casetotals.head()

# sort the land temperature data and drop rows with missing values for temperature

ltbrazil  = ltbrazil.sort_values(['station', 'month'])
ltbrazil = ltbrazil.dropna(subset = ['temperature'])

# use a numpy array to  calculate average temperature for the year

# exclude rows where theree is a large change from one period to the next

prevstation = "ZZZ"
prevtemp = 0
rowlist = []

tempvalues = ltbrazil[['station', 'temperature']].to_numpy()

for j in range(len(tempvalues)):
  station = tempvalues[j][0]
  temperature = tempvalues[j][1]
  if (prevstation != station):
    if (prevstation != 'ZZZ'):
      rowlist.append({'station':prevstation, 'temperature' : tempcnt/stationcnt, 'statiocnt': stationcnt})
    tempcnt = 0
    stationcnt = 0
    prevstation = station
  if ((0<=abs(temperature - prevtemp)<=3) or (stationcnt == 0)):
    tempcnt += temperature
    stationcnt +=1
  prevtemp = temperature

rowlist.append({'station':prevstation, 'avgtemp':tempcnt/stationcnt ,'stationcnt':stationcnt})
rowlist[0:5]

# create a data frame of the land temperature average

ltbrazilavgs = pd.DataFrame(rowlist)
ltbrazilavgs.head()



"""# **Using group by to organize data by groups**"""

import pandas as pd
import numpy as np
coviddaily = pd.read_csv("coviddaily720.csv", parse_dates = ['casedate'])

# create a pandas groupby DataFrame

contrytots = coviddaily.groupby(['location'])
contrytots

type(contrytots)

contrytots.head(5)

# create a dataframe for the first and last rows of each country

contrytots.first().iloc[0:5, 0:5]

contrytots.last().iloc[0:5, 0:5]

contrytots.get_group('Zimbabwe').iloc[0:5, 0:5]

# loop through groups

for name, group in contrytots:           # name = country name # group = correspond country group
  if (name in ['Malta', 'Kuwait']):
    print(group.iloc[0:5,0:5])

# show the number of rows for each country

contrytots.size()

# Show the summary statisics by country

contrytots.new_cases.describe().head()

# These steps demonstrate how remarkably useful the groupby DataFrame object is when we want to
#generate summary statistics by categorical variables

"""# **Using more complicated aggregation functions with group by**"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv("nls97b.csv")
nls97.set_index('personid', inplace = True)

# Review the structure of the data

nls97.iloc[:, 0:7].info()

# review some of the categorical data

catvars = ['gender', 'maritalstatus', 'highestdegree']

for col in catvars:
  print(col, nls97[col].value_counts().sort_index(), sep = '\n\n', end = "\n\n\n")

# Review some decriptive statistics

contvars = ['satmath', 'satverbal', 'weeksworked06', 'gpaoverall', 'childathome']

nls97[contvars].describe()

# look at scholastic Assesment Test(SAT) math score by gender
# we pass the column name to groupby to group by that column

nls97.groupby('gender')['satmath'].mean()

# look at the SAT math scores by gender and highest degree earned

# we can pass a list of column names to groupby to group by more  than one column

nls97.groupby(['gender', 'highestdegree'])['satmath'].mean()

# look at the SAT math and verbal scores by gender and highest degree earned

# We can use a list to summarize values for more than one variable, in this case satmath and
# satverbal:

nls97.groupby(['gender', 'highestdegree'])[['satmath', 'satverbal']].mean()

# add columns for the count, max, and standard deviation

# use the agg function to return several summary statistics

nls97.groupby(['gender', 'highestdegree'])['gpaoverall'].agg(['count','mean', 'max', 'std'])

# use a dictionary for more complicated aggregation

pd.options.display.float_format = '{:,.1f}'.format
aggdict = {'weeksworked06':['count', 'mean', 'max', 'std'], 'childathome':['count', 'mean','max', 'std']}
nls97.groupby(['highestdegree']).agg(aggdict)

nls97.groupby(['maritalstatus']).agg(aggdict)



"""# **Using user-defined functions and apply with groupby**"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv("nls97b.csv")
nls97.set_index('personid', inplace = True)

# create a fucntion for defining the interquartile range

def iqr(x):
  return x.quantile(0.75)-x.quantile(0.25)

# run the intequartile range function

# First, create a dictionary that specifies which aggregation functions to run on each analysis variable:

aggdict = {'weeksworked06':['count', 'mean', iqr], 'childathome':['count', 'mean', iqr]}
nls97.groupby(['highestdegree']).agg(aggdict)

# define the function to return selected summary statistics as a series
def gettots(x):
  out = {}
  out['qr1'] = x.quantile(0.25)
  out['med'] = x.median()
  out['qr3'] = x.quantile(0.75)
  out['count'] = x.count()
  return pd.Series(out)

# use apply to run the function

# This will create a series with a multi-index based on highestdegree values and the desired
# summary statistics:

pd.options.display.float_format = '{:,.0f}'.format
nls97.groupby(['highestdegree'])['weeksworked06'].apply(gettots)

# use reset_index to use the default index instead of the index created from the  groupby Dataframe
nls97.groupby(['highestdegree'])['weeksworked06'].apply(gettots).reset_index()

# chain with the unstack instead to create columns bases on the summary statistics
# This will create a series with a multi-index based on highestdegree values and the desired
# summary statistics:

nlssumns = nls97.groupby(['highestdegree'])['weeksworked06'].apply(gettots).unstack()
nlssumns

"""# **Using groupby to change the unit of analysis of a Data Frame**"""

import pandas as pd
coviddaily = pd.read_csv("coviddaily720.csv", parse_dates = ['casedate'])
ltbrazil  = pd.read_csv("ltbrazil.csv")

# convert covid data from one country per day to summaries across all countries by day

coviddailytotals = coviddaily.loc[coviddaily.casedate.between('2020-02-01', '2020-07-12')].\
                    groupby(['casedate'], as_index = False)[['new_cases', 'new_deaths']].sum()
coviddailytotals.head()

# Create a DataFrme with average temperatures for each station in Brazil
# first remove rows with missing temperature values, and show some data for a few rows

ltbrazil = ltbrazil.dropna(subset=['temperature'])
ltbrazil.loc[103508:104551,['station','year','month','temperature','elevation','latabs']]

ltbrazilavgs = ltbrazil.groupby(['station'], as_index = False).agg({'latabs':'first', 'elevation':'first', 'temperature':'mean'})
ltbrazilavgs.head(10)



"""<h1 align = "center"><b>Chapter 08</b></h2>

# **08. Addressing Data issues when combining DataFrames**

# **Combining DataFrames Vertically**
"""

import pandas as pd
import numpy as np
import os

# Load the data from Cmaroon and poland

ltcameroon = pd.read_csv("ltcameroon.csv")
ltpoland = pd.read_csv("ltpoland.csv")

# concatenate the cameroon and poland data

ltcameroon.shape

ltpoland.shape

ltall = pd.concat([ltcameroon, ltpoland])
ltall.country.value_counts()

# Concatenation all the country data file

# #Loop through all the filenames in the folder that contains the CSV files for each country. Use the
# endswith method to check that the filename has a CSV file extension. Use read_csv to
# create a new DataFrame and print out the number of rows. Use concat to append the rows of the
# new DataFrame to the rows that have already been appended. Finally, display any columns that are missing
# in the most recent DataFrame, or that are in the most recent DataFrame but not the previous ones. Notice
# that the ltoman DataFrame is missing the latabs column:


directory = "data/ltcountry"  # need to include directoru location ex. data/ltcountry
ltall = pd.DataFrame()

for filename in os.listdir(directory):
  if filename.endswith(".csv"):
    fileloc = os.path.join(directory, filename)
    # open the next file
    with open(fileloc) as f:
      ltnew = pd.read_csv(fileloc)
      print(filename + " has "+str(ltnew.shape[0])+"rows .")
      ltall = pd.concat([ltall, ltnew])
      # check the differences in columns
      columndiff = ltall.columns.symmetric_difference(ltnew.columns)
      if (not columndiff.empty):
        print("", "Different columns names for :", filename, columndiff, "", sep = "\n")

# show some combined data

ltall[['country', 'station', 'month', 'temperature', 'latitude']].sample(5, random_state = 1)

# Check the values in the concatenated data

# Notice that the values for latabs for Oman are all missing. This is because latabs is missing
# in the DataFrame for Oman (latabs is the absolute value of the latitude for each station):

ltall.country.value_counts().sort_index()

# fix the missing values

# Set the value of latabs to the value of latitude for Oman. (All of the latitude
# values for stations in Oman are above the equator and positive. In the Global Historical Climatology Network
# integrated database, latitude values above the equator are positive, while all the latitude values
# below the equator are negative). Do this as follows

ltall['latabs'] = np.where(ltall.country == 'Oman', ltall.latitude, ltall.latabs)
ltall.groupby(['country']).agg({'temperature':['min','mean', 'max', 'count'], 'latabs': ['min', 'mean', 'max', 'count']})



"""# **Doing one-to-one merges**"""

import pandas as  pd

nls97 = pd.read_csv('nls97f.csv')
nls97.set_index('personid',inplace = True)
nls97add = pd.read_csv("nls97add.csv")

# look at the some of the NLS data

nls97.head()

nls97add.head()

nls97.shape

nls97add.shape

# check the number of unique values for originalid is equal to the number of rows
 # we will use originalid for our merge-by column  later

nls97.originalid.nunique() == nls97.shape[0]

nls97add.originalid.nunique() == nls97add.shape[0]

# create some mismatch ids
# Unfortunately, the NLS data is a little too clean for our purposes. Due to this, we will mess up a couple of
# values for originalid. originalid is the last column in the nls97 file and the
# first column in the nls97add file:

nls97 = nls97.sort_values('originalid')
nls97add = nls97add.sort_values('originalid')

nls97.loc[nls97.index[0:2], 'originalid'] += 10000
nls97.originalid.head(2)

nls97add.iloc[0:2, 0] += 20000

nls97add.head(2)

# use join to perform a left join

# nls97 is the left DataFrame and nls97add is the right DataFrame when we use join in
# this way. Show the values for the mismatched IDs. Notice that the values for the columns from the right
# DataFrame are all missing when there is no matching ID on that DataFrame (the orignalid
# values 10001 and 10002 appear on the left DataFrame but not on the right DataFrame):

nlsneww = nls97.join(nls97add.set_index(['originalid']))
nlsneww.loc[nlsneww.originalid > 9999, ['originalid', 'gender', 'birthyear', 'motherage', 'parentincome']]

nlsneww.head(3)

# Perform a left join with merge

#  nls97 is the left DataFrame and nls97add is the right DataFrame when we use join in
# this way. Show the values for the mismatched IDs. Notice that the values for the columns from the right
# DataFrame are all missing when there is no matching ID on that DataFrame (the orignalid
# values 10001 and 10002 appear on the left DataFrame but not on the right DataFrame):

nlsnew = pd.merge(nls97, nls97add, on= ['originalid'], how = "left")
nlsnew.loc[nlsnew.originalid > 9999, ['originalid', 'gender', 'birthyear', 'motherage', 'parentincome']]

nlsnew.head(3)

# perform right join

# With a right join, the values from the left DataFrame are missing when there is no matching ID on the left
# DataFrame:

nls97new = pd.merge(nls97, nls97add, on = ['originalid'], how = "right")
nls97new.loc[nls97new.originalid >9999, ['originalid', 'gender', 'birthyear', 'motherage', 'parentincome']]

nls97new.head(3)

# perform inner join

# With a right join, the values from the left DataFrame are missing when there is no matching ID on the left
# DataFrame:

nls97inner = pd.merge(nls97, nls97add, on = ['originalid'], how = "inner")
nls97inner.loc[nls97inner.originalid > 9999, ['originalid', 'gender', 'birthyear', 'motherage', 'parentincome']]

# return the empty dataFrame

nls97inner.head(3)

# perform outer join

# This retains all the rows, so rows with merge-by values in the left DataFrame but not in the right are retained
# (originalid values 10001 and 10002), and rows with merge-by values in the right
# DataFrame but not in the left are also retained (originalid values 20001 and 20002):

nls97outer = pd.merge(nls97, nls97add, on = ['originalid'], how = "outer")
nls97outer.loc[nls97outer.originalid > 9999, ['originalid', 'gender', 'birthyear', 'motherage', 'parentincome']]

nls97outer.head(3)

# Create a function to check for ID mismatches

# The function takes a left and right DataFrame, as well as a merge-by column. It perform an outer join
# because we want to see which merge-by values are present in either DataFrame, or both of them:

def checkmerge(dfleft, dfright, idvar):
  dfleft['inleft'] = "Y"
  dfright['inright'] = "Y"
  dfboth = pd.merge(dfleft[[idvar, 'inleft']], dfright[[idvar, 'inright']], on = [idvar], how = "outer")
  dfboth.fillna("N", inplace = True)
  print(pd.crosstab(dfboth.inleft, dfboth.inright))

checkmerge(nls97, nls97add, "originalid")



"""# **Using multiple merge-by columns**

"""

import pandas as pd

nls97weeksworked = pd.read_csv("nls97weeksworked.csv")
nls97colenr = pd.read_csv("nls97colenr.csv")

nls97weeksworked.head(4)

nls97colenr.head(4)

nls97weeksworked.shape

nls97weeksworked.originalid.nunique()

nls97colenr.shape

nls97colenr.originalid.nunique()

# Check for unique values in the merge-by columns

nls97weeksworked.groupby(['originalid', 'year'])['originalid'].count().shape

nls97colenr.groupby(['originalid', 'year'])['originalid'].count().shape

# Check the mismatches in the merge-by columns

def checkmerges(dfleft, dfright, idvar):
  dfleft['inleft'] = "Y"
  dfright['inright'] = "Y"
  dfboth = pd.merge(dfleft[idvar + ['inleft']], dfright[idvar+ ['inright']],  on = idvar, how = "outer")
  dfboth.fillna('N', inplace = True)
  print(pd.crosstab(dfboth.inleft, dfboth.inright))


checkmerges(nls97weeksworked.copy(), nls97colenr.copy(), ['originalid','year'])

# Perform a merge with multiple merge-by columns

nlsworkschool = pd.merge(nls97weeksworked, nls97colenr, on = ['originalid', 'year'], how = 'inner')
nlsworkschool.shape

nlsworkschool.head(4)

"""# **Doing one to many merges**"""

import pandas as pd
import numpy as np

countries = pd.read_csv('ltcountries.csv')
locations = pd.read_csv('ltlocations.csv')

countries.head(4)

locations.head(4)

locations.sample(4, random_state = 1)

# set the index for the wheather station(locations) and country data

# confirm that the merge-by values for the countries DataFrame are unique

countries.set_index(['countryid'], inplace = True)
locations.set_index(['countryid'], inplace = True)

countries.head(4)

locations.head(4)

countries.index.nunique() == countries.shape[0]

locations[['locationid', 'latitude', 'stnelev']].head(10)

# performe left join of countries and locations using join

stations = countries.join(locations)
stations[['locationid', 'latitude', 'stnelev', 'country']].head(10)

# check the merge by columns same

stations

#  check that the merge-by column matches

# First, reload the DataFrames since we have made some changes. The checkmerge function
# shows that there are 27,472 rows with merge-by values (from countryid) in both DataFrames and
# two in countries (the left DataFrame) but not in locations. This indicates that an inner
# join would return 27,472 rows and a left join would return 27,474 rows. The last statement in the function
# identifies the countryid values that appear in one DataFrame but not the other:

countries = pd.read_csv("ltcountries.csv")
locations = pd.read_csv("ltlocations.csv")


def checkmerge(dfleft, dfright, idvars):
  dfleft['inleft'] = "Y"
  dfright['inright'] = "Y"
  dfboth = pd.merge(dfleft[[idvars, "inleft"]], dfright[[idvars, "inright"]], on = [idvars], how = "outer")
  dfboth.fillna("N", inplace = True)
  print(pd.crosstab(dfboth.inleft, dfboth.inright))
  print(dfboth.loc[(dfboth.inleft == "N") | (dfboth.inright == "N")])

checkmerge(countries.copy(), locations.copy(), "countryid")

# show the rows in one file but not the other

# The last statement in the previous step displays the two values of countryid in
# countries but not in locations, and the one in locations but not in
# countries

countries.loc[countries.countryid.isin(["LQ", "ST"])]

locations.loc[locations.countryid == "FO"]

# merge the locations and countries DataFrame

# Perform a left join. Also, count the number of missing values for each column, where merge-by values are
# present in the countries data but not in the weather station data:

stations = pd.merge(countries, locations, on= ["countryid"], how = "left")
stations[['locationid', 'latitude', 'stnelev', 'country']].head(10)

stations.shape

stations.loc[stations.countryid.isin(["LQ", "ST"])].isnull().sum()

#

"""# **Doing one to many merges**"""

import pandas as pd

cmacitations = pd.read_csv("cmacitations.csv")
cmscreations = pd.read_csv("cmacreators.csv")

cmacitations.head(4)

cmscreations.head(4)

# look at the creators data
cmscreations.loc[:,['id', 'creator', 'birth_year']].head(10)

# shows the duplicates of merge-by values in the citations data

cmscreations.shape

cmscreations.id.nunique()

# there are 174 media citations for collection item  148758

cmacitations.id.value_counts().head(10)

cmscreations.id.value_counts().head(10)

# Check the merge
# Use the checkmerge function we used in the Doing one-to-many merges recipe:

def checkmerge(dfleft, dfright, idvar):
  dfleft['inleft'] = "Y"
  dfright['inright'] = "Y"
  dfboth = pd.merge(dfleft[[idvar , 'inleft']], dfright[[idvar, 'inright']] , on = idvar, how= "outer")
  dfboth.fillna("N", inplace = True)
  print(pd.crosstab(dfboth.inleft, dfboth.inright))


checkmerge(cmacitations.copy(), cmscreations.copy(), "id")

# show the merge-by values duplicates in both DataFrame

cmacitations.loc[cmacitations.id == 124733]

# do a many to many merges

cma = pd.merge(cmacitations, cmscreations, on = ['id'], how = "outer")
cma['citation'] = cma.citation.str[0:20]
cma['creator'] = cma.creator.str[0:20]
cma.loc[cma.id == 124733, ['citation', 'creator', 'birth_year']]



"""<h1 align = "center"><b>Chapter 09</b></h1>"""



"""# **09. Tidying and Reshaping Data**

# **Removing Duplicated Rows**
"""

import pandas as pd

covidcases = pd.read_csv("covidcases720.csv")

covidcases.head(4)

# create list for the daily cases and deaths columns the case total columns, and the demographic columns

dailyvars = ['casedate', 'new_cases', 'new_deaths']
totvars = ['location', 'total_cases', 'total_deaths']
demovars = ['population', 'population_density', 'median_age', 'gdp_per_capita', 'hospital_beds_per_thousand', 'region']

covidcases[dailyvars+ totvars+ demovars].head(3).T

# create the DataFrame with just the daily data

coviddaily = covidcases[['location'] + dailyvars]
coviddaily.shape

coviddaily.head()

# Select one row per country

# Check to see how many countries (location) to expect by getting the number of unique locations.
# Sort by location and casedate. Then use drop_duplicates to select one
# row per location, and use the keep parameter to indicate that we want the last row for each
# country:

covidcases.location.nunique()

coviddemo = covidcases[['casedate'] + totvars + demovars].sort_values(['location', 'casedate'])\
            .drop_duplicates(['location'], keep = "last").rename(columns = {'casedate':'lastdate'})

coviddemo.shape

coviddemo.sample(4, random_state = 1  )

# sum the values for the each group

# Use the pandas DataFrame groupby method to sum total cases and deaths for each country. Also,
# get the last value for some of the columns that are duplicated across all rows for each country:
# median_age, gdp_per_capita, region, and casedate. (We select
# only a few columns from the DataFrame.) Notice that the numbers match those from step 4

covidtotals = covidcases.groupby(['location'], as_index = False).agg({'new_cases':'sum', 'new_deaths': 'sum', \
                                                                      'median_age':'last', 'gdp_per_capita':'last',\
                                                                      'region': 'last', 'casedate':'last','population':'last'})\
                        .rename(columns = {'new_cases':'total_cases', 'new_deaths':'total_deaths', 'casedate':'lastdate '})


covidtotals.head(3).T



"""# **Fixing many-to-many relationship**"""

import pandas as pd

cma = pd.read_csv('cmacollections.csv')
cma.head()

cma.shape

cma.id.nunique()

cma.drop_duplicates(['id', 'citation']).id.count()

cma.drop_duplicates(['id', 'creator']).id.count()

#show the collection item with duplicated citation and creators.
cma.set_index(['id'], inplace = True)

cma.loc[124733,['title', 'citation', 'creator', 'birth_year',]].head(4).T

# create collection dataFrame

collectionsvars = ['title', 'collection', 'type']
cmacollections = cma[collectionsvars].reset_index().drop_duplicates(['id']).set_index(['id'])

cmacollections.head(4)

cmacollections.shape

# create citation DataFrame
# This will just have the id  and the citation

cmacitations = cma[['citation']].reset_index().drop_duplicates(['id', 'citation']).set_index(['id'])
cmacitations.loc[124733]

# create creator DataFrame

creatorsvars = ['creator', 'birth_year', 'death_year']
cmacreators = cma[creatorsvars].reset_index().drop_duplicates(['id', 'creator']).set_index(['id'])

cmacreators.loc[124733]

# count the number of collection items with a creator born after 1950
# First, convert the birth_year values from string to numeric. Then create a DataFrame with just
# young artists. Finally, merge that DataFrame with the collections DataFrame to create a flag for collection
# items that have at least one creator born after 1950:

cmacreators['birth_year'] = cmacreators.birth_year.str.findall("\d+").str[0].astype(float)
youngartist = cmacreators.loc[cmacreators.birth_year > 1950, ['creator']].assign(creatorbornafter1950 = 'Y')
youngartist.shape[0] == youngartist.index.nunique()

youngartist

cmacollection = pd.merge(cmacollections, youngartist, left_on = ['id'], right_on = ['id'], how= 'left')

cmacollection.creatorbornafter1950.fillna("N", inplace = True)
cmacollection.head(5)

cmacollection.shape

cmacollection.creatorbornafter1950.value_counts()



"""# Using stack and melt to reshape data from wide to long format"""

import pandas as pd

nls97 = pd.read_csv('nls97f.csv')

nls97.head(4)

# view some of the values for the number of weeks worked

nls97.set_index(['originalid'], inplace = True)

weeksworkedcols = ['weeksworked00', 'weeksworked01', 'weeksworked02', 'weeksworked03', 'weeksworked04']
nls97[weeksworkedcols].head(4).T

nls97.shape

# use to stack to transform the data from wide to long

  # First, select only the weeksworked## columns. Use stack to move each column name in
  # the original DataFrame into the index and move the weeksworked## values into the associated
  # row. Reset the index so that the weeksworked## column names become the values for the
  # level_0 column (which we rename year), and the weeksworked## values become
  # the values for the 0 column (which we rename weeksworked):

weeksworked = nls97[weeksworkedcols].stack(dropna = False).reset_index().rename(columns = {'level_1': 'year', 0 : 'weeksworked'})
weeksworked.head(10)

# fix the year values
  # Get the last digits of the year values, convert them to integers , and add 2000

weeksworked['year'] = weeksworked.year.str[-2:].astype(int) + 2000

weeksworked.head(10)

weeksworked.shape

# alternatively,use melt to transform data from wide to long

  # First, reset the index and select the originalid and weeksworked## columns. Use
  # the id_vars and value_vars parameters of melt to specify originalid as
  # the ID variable and the weeksworked## columns as the columns to be rotated, or melted. Use
  # the var_name and value_name parameters to rename the columns to year and
  # weeksworked respectively. The column names in value_vars become the values for
  # the new year column (which we convert to an integer using the original suffix). The values for the
  # value_vars columns are assigned to the new weeksworked column for the associated
  # row:

weeksworked = nls97.reset_index().loc[:, ['originalid'] + weeksworkedcols]\
              .melt(id_vars = ['originalid'],value_vars = weeksworkedcols, var_name = 'year',\
                    value_name = 'weeksworked')
weeksworked

weeksworked['year'] = weeksworked.year.str[-2:].astype(int) + 2000
weeksworked.set_index(['originalid'], inplace = True)
weeksworked.loc[[8245,3962]]

# Reshape the college enrollement columns with melt
 #This works the same way as the melt function for the weeks worked columns:

colenrcols = ['colenroct00', 'colenroct01', 'colenroct02', 'colenroct03', 'colenroct04']

colenr = nls97.reset_index().loc[:, ['originalidm'] + colenrcols]\
        .melt(id_vars = ['originalid'],value_vars = colenrcols, var_name = 'year', value_name = 'colenr')

colenr

colenr['year'] = colenr.year.str[-2:].astype(int) + 2000
colenr.set_index(['originalid'], inplace = True)
colenr.loc[[8245, 3962]]

# merge the weeks worked and college enrollement data

workschool = pd.merge(weeksworked, colenr ,on=['originalid','year'], how="inner")
workschool.shape

workschool.loc[[8245, 3962]]



"""# **Melting Multiple groups of columns**

When we needed to melt multiple groups of columns in the previous recipe, we used melt twice and then
merged the resulting DataFrames. That worked fine, but we can accomplish the same tasks in one step with
the wide_to_long function. wide_to_long has more functionality than melt, but
is a bit more complicated to use.
"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv("nls97f.csv")
nls97.set_index('personid', inplace = True)

# view some of the weeks worked and college enrollment data

weeksworkedcols = ['weeksworked00', 'weeksworked01', 'weeksworked02', 'weeksworked03', 'weeksworked04']
colenrcols = ['colenroct00', 'colenroct01', 'colenroct02', 'colenroct03', 'colenroct04']

nls97.loc[nls97.originalid.isin([1,2]),['originalid'] + weeksworkedcols + colenrcols].T

# run the wide-to-long function
  # When we needed to melt multiple groups of columns in the previous recipe, we used melt twice and then
  # merged the resulting DataFrames. That worked fine, but we can accomplish the same tasks in one step with
  # the wide_to_long function. wide_to_long has more functionality than melt, but
  # is a bit more complicated to use.

workschool = pd.wide_to_long(nls97[['originalid'] + weeksworkedcols + colenrcols], \
                              stubnames = ['weeksworked', 'colenroct'], i = ['originalid'], \
                              j = 'year').reset_index()

workschool.head()

workschool['year'] = workschool.year + 2000
workschool = workschool.sort_values(['originalid', 'year'])
workschool.set_index(['originalid'], inplace = True)
workschool.head(10)

# When we needed to melt multiple groups of columns in the previous recipe, we used melt twice and then
# merged the resulting DataFrames. That worked fine, but we can accomplish the same tasks in one step with
# the wide_to_long function. wide_to_long has more functionality than melt, but
# is a bit more complicated to use.

weeksworkedcols = ['weeksworked00', 'weeksworked01', 'weeksworked02', 'weeksworked04', 'weeksworked05']
workschool = pd.wide_to_long(nls97[['originalid'] + weeksworkedcols + colenrcols], stubnames = ['weeksworked', 'colenroct'], i = ['originalid'], j = 'year').reset_index()

workschool['year'] = workschool.year + 2000
workschool = workschool.sort_values(['originalid', 'year'],)
workschool.set_index(['originalid'], inplace = True)
workschool.head(12)

"""The weeksworked values for 2003 are now missing, as are the colenroct values for
2005. (The weeksworked value for 2002 for originalid 1 was already missing.)
"""



"""# Using unstack and pivot to reshape data from long to wide

ometimes, we actually have to move data from a tidy to an untidy structure. This is often because we need to
prepare the data for analysis with software packages that do not handle relational data well, or because we
are submitting data to some external authority that has requested it in an untidy format. unstack and
pivot can be helpful when we need to reshape data from long to wide format. unstack does the
opposite of what we did with stack, and pivot does the opposite of melt
"""

import pandas as pd
import numpy as np

nls97 = pd.read_csv('nls97f.csv')
nls97.set_index('originalid', inplace = True)

# stack the data again
# This repeats the stack operation from an earlier recipe in this chapter:

weeksworkedcols = ['weeksworked00', 'weeksworked01', 'weeksworked02', 'weeksworked03', 'weeksworked04']
weeksworkedstacked = nls97[weeksworkedcols].stack(dropna = False)
weeksworkedstacked.loc[[1,2]]

# melt the data again

# This repeats the stack operation from an earlier recipe in this chapter:

weeksworkedmelted = nls97.reset_index().loc[:,['originalid'] + weeksworkedcols].melt(id_vars = ['originalid'], \
                                                                                     value_vars = weeksworkedcols,\
                                                                                     var_name = 'year',\
                                                                                     value_name = 'weeksworked')

weeksworkedmelted.loc[weeksworkedmelted.originalid.isin([1,2])].sort_values(['originalid', 'year'])

# use unstack  to convert the stacked data from long to wide

weeksworked = weeksworkedstacked.unstack()

weeksworked.loc[[1,2]]

# use povot to convert the melted data from long to wide

# pivot is slightly more complicated than unstack. We need to pass arguments to do the reverse
# of melt, telling pivot the column to use for the column name suffixes (year) and where to
# grab the values to be unmelted (from the weeksworked columns, in this case):

weeksworked = weeksworkedmelted.pivot(index = 'originalid', columns ='year', \
                                      values = ['weeksworked']).reset_index()
weeksworked

weeksworked.columns = ['originalid'] + [col[1]  for col in weeksworked.columns[1:]]
weeksworked.columns

# print(['originalid'] + [col[1]])
weeksworked.columns[1:]

weeksworked

weeksworked.loc[weeksworked.original  id.isin([1,2])].T